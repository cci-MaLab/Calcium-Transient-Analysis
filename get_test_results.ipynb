{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract test results\n",
    "\n",
    "The following notebook is to extrapolate the data used in testing in the ./ml_training/test_results/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set to True if you don't have the pickle file (LEAVE THIS FALSE IF YOU HAVE THE PICKLE FILE)\n",
    "if False:\n",
    "    results_path = \"./ml_training/test_results/\"\n",
    "\n",
    "    # Iterate through the results files and load the folders into dicts\n",
    "    results = {}\n",
    "    entries = os.listdir(results_path)\n",
    "    for entry in tqdm(entries):\n",
    "        if os.path.isdir(results_path + entry):\n",
    "            results[entry] = {}\n",
    "            sub_entries = os.listdir(results_path + entry)\n",
    "            for sub_entry in sub_entries:\n",
    "                if os.path.isdir(results_path + entry + \"/\" + sub_entry):\n",
    "                    results[entry][sub_entry] = {}\n",
    "                    sub_sub_entries = os.listdir(results_path + entry + \"/\" + sub_entry)\n",
    "                    # Extract the one file with .csv extension\n",
    "                    for sub_sub_entry in sub_sub_entries:\n",
    "                        if sub_sub_entry.endswith(\".csv\"):\n",
    "                            final_path = results_path + entry + \"/\" + sub_entry + \"/\" + sub_sub_entry\n",
    "                            results[entry][sub_entry] = np.genfromtxt(final_path, delimiter=\",\", dtype=float)\n",
    "                            # Greater than 0.5 is 1 result else 0\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] >= 0.5] = 1\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] < 0.5] = 0\n",
    "                            results[entry][sub_entry] = results[entry][sub_entry].astype(np.int8)\n",
    "                            break\n",
    "\n",
    "    # Save the dict with pickle\n",
    "    with open(\"test_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_animal\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_cross_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_same_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_session_same_day\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "within_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\n"
     ]
    }
   ],
   "source": [
    "results = pickle.load(open(\"test_results.pkl\", \"rb\"))\n",
    "# Print keys and subkeys\n",
    "for key in results.keys():\n",
    "    print(key)\n",
    "    for subkey in results[key].keys():\n",
    "        print(\"\\t\" + subkey)\n",
    "    print()\n",
    "\n",
    "print(\"The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes in three arguments, the experiment type, the training data and a function to apply to the data.\n",
    "# You can omit providing the training data and it will accumulate the results for all subkeys in the results dict.\n",
    "\n",
    "def apply_to_results(experiment_type, func, training_name=None, average=False):\n",
    "    if experiment_type not in results.keys():\n",
    "        raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "    if training_name is not None:\n",
    "        if training_name not in results[experiment_type].keys():\n",
    "            raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "        training_name = [training_name]\n",
    "    else:\n",
    "        training_name = results[experiment_type].keys()\n",
    "    \n",
    "    # We need to extract the results. It is set up in the following manner:\n",
    "    # Each two rows represent the ground truth and predictions respectively. There should be 60 rows because each experiment was repeated 5 times, for values [1, 2, 5, 10, 15, 20]. So 2*5*6=60\n",
    "    # Therefore the func should be applied to each pair of rows.\n",
    "\n",
    "    metrics = {}\n",
    "    for name in training_name:\n",
    "        arr = results[experiment_type][name]\n",
    "        for i, size in enumerate([1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 20, 20, 20, 20, 20]):\n",
    "            ground_truth = arr[i*2]\n",
    "            predictions = arr[i*2+1]\n",
    "            if metrics.get(size) is None:\n",
    "                metrics[size] = []\n",
    "            metrics[size].append(func(ground_truth, predictions))\n",
    "    \n",
    "    if average:\n",
    "        for key in metrics.keys():\n",
    "            metrics[key] = np.mean(metrics[key])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def true_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def true_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def false_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "def false_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "\"\"\"\n",
    "def transient_overlap(y_true, y_pred):\n",
    "    # We'll do the following, set ground truth to 2 and predictions to 1.\n",
    "    sum_arr = y_true + y_pred\n",
    "    # Find stretches where the result is non-zero\n",
    "    diff_arr = np.diff(sum_arr[sum_])\n",
    "\"\"\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = lambda x, y: f1_score(x, y, average=\"macro\")\n",
    "f1_standard = lambda x, y: f1_score(x, y)\n",
    "tp = lambda x, y: true_positives(x, y)\n",
    "tn = lambda x, y: true_negatives(x, y)\n",
    "fp = lambda x, y: false_positives(x, y)\n",
    "fn = lambda x, y: false_negatives(x, y)\n",
    "precision = lambda x, y: precision_score(x, y)\n",
    "recall = lambda x, y: recall_score(x, y)\n",
    "accuracy = lambda x, y: accuracy_score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntp_result = apply_to_results(\"within_session\", tp, average=True)\\ntn_result = apply_to_results(\"within_session\", tn, average=True)\\nfp_result = apply_to_results(\"within_session\", fp, average=True)\\nfn_result = apply_to_results(\"within_session\", fn, average=True)\\nprecision_result = apply_to_results(\"within_session\", precision, average=True)\\nrecall_result = apply_to_results(\"within_session\", recall, average=True)\\naccuracy_result = apply_to_results(\"within_session\", accuracy, average=True)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "f1_result_macro = apply_to_results(\"cross_animal\", f1_macro, training_name=\"PL010_D1S1\", average=True)\n",
    "f1_result_standard = apply_to_results(\"cross_animal\", f1_standard, training_name=\"PL010_D1S1\", average=True)\n",
    "\"\"\"\n",
    "tp_result = apply_to_results(\"within_session\", tp, average=True)\n",
    "tn_result = apply_to_results(\"within_session\", tn, average=True)\n",
    "fp_result = apply_to_results(\"within_session\", fp, average=True)\n",
    "fn_result = apply_to_results(\"within_session\", fn, average=True)\n",
    "precision_result = apply_to_results(\"within_session\", precision, average=True)\n",
    "recall_result = apply_to_results(\"within_session\", recall, average=True)\n",
    "accuracy_result = apply_to_results(\"within_session\", accuracy, average=True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 macro\n",
      "{1: 0.5480958946232736, 2: 0.744130759185736, 5: 0.8227125097213384, 10: 0.8558539053757613, 15: 0.900605819452913, 20: 0.902093478833422}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"f1 standard\")\\nprint(f1_result_standard)\\nprint(\"True Positives Average\")\\nprint(tp_result)\\nprint(\"True Negatives Average\")\\nprint(tn_result)\\nprint(\"False Positives Average\")\\nprint(fp_result)\\nprint(\"False Negatives Average\")\\nprint(fn_result)\\nprint(\"Precision Average\")\\nprint(precision_result)\\nprint(\"Recall Average\")\\nprint(recall_result)\\nprint(\"Accuracy Average\")\\nprint(accuracy_result)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"f1 macro\")\n",
    "print(f1_result_macro)\n",
    "print(\"f1 standard\")\n",
    "print(f1_result_standard)\n",
    "\"\"\"\n",
    "print(\"True Positives Average\")\n",
    "print(tp_result)\n",
    "print(\"True Negatives Average\")\n",
    "print(tn_result)\n",
    "print(\"False Positives Average\")\n",
    "print(fp_result)\n",
    "print(\"False Negatives Average\")\n",
    "print(fn_result)\n",
    "print(\"Precision Average\")\n",
    "print(precision_result)\n",
    "print(\"Recall Average\")\n",
    "print(recall_result)\n",
    "print(\"Accuracy Average\")\n",
    "print(accuracy_result)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_exploration_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
