{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract test results\n",
    "\n",
    "The following notebook is to extrapolate the data used in testing in the ./ml_training/test_results/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set to True if you don't have the pickle file (LEAVE THIS FALSE IF YOU HAVE THE PICKLE FILE)\n",
    "if False:\n",
    "    results_path = \"./ml_training/test_results/\"\n",
    "\n",
    "    # Iterate through the results files and load the folders into dicts\n",
    "    results = {}\n",
    "    entries = os.listdir(results_path)\n",
    "    for entry in tqdm(entries):\n",
    "        if os.path.isdir(results_path + entry):\n",
    "            results[entry] = {}\n",
    "            sub_entries = os.listdir(results_path + entry)\n",
    "            for sub_entry in sub_entries:\n",
    "                if os.path.isdir(results_path + entry + \"/\" + sub_entry):\n",
    "                    results[entry][sub_entry] = {}\n",
    "                    sub_sub_entries = os.listdir(results_path + entry + \"/\" + sub_entry)\n",
    "                    # Extract the one file with .csv extension\n",
    "                    for sub_sub_entry in sub_sub_entries:\n",
    "                        if sub_sub_entry.endswith(\".csv\"):\n",
    "                            final_path = results_path + entry + \"/\" + sub_entry + \"/\" + sub_sub_entry\n",
    "                            results[entry][sub_entry] = np.genfromtxt(final_path, delimiter=\",\", dtype=float)\n",
    "                            # Greater than 0.5 is 1 result else 0\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] >= 0.5] = 1\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] < 0.5] = 0\n",
    "                            results[entry][sub_entry] = results[entry][sub_entry].astype(np.int8)\n",
    "                            break\n",
    "\n",
    "    # Save the dict with pickle\n",
    "    with open(\"test_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_animal\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_cross_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_same_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_session_same_day\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "within_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\n"
     ]
    }
   ],
   "source": [
    "results = pickle.load(open(\"test_results.pkl\", \"rb\"))\n",
    "# Print keys and subkeys\n",
    "for key in results.keys():\n",
    "    print(key)\n",
    "    for subkey in results[key].keys():\n",
    "        print(\"\\t\" + subkey)\n",
    "    print()\n",
    "\n",
    "print(\"The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes in three arguments, the experiment type, the training data and a function to apply to the data.\n",
    "# You can omit providing the training data and it will accumulate the results for all subkeys in the results dict.\n",
    "\n",
    "def apply_to_results(experiment_type, func, training_name=None, average=False):\n",
    "    if experiment_type not in results.keys():\n",
    "        raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "    if training_name is not None:\n",
    "        if training_name not in results[experiment_type].keys():\n",
    "            raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "        training_name = [training_name]\n",
    "    else:\n",
    "        training_name = results[experiment_type].keys()\n",
    "    \n",
    "    # We need to extract the results. It is set up in the following manner:\n",
    "    # Each two rows represent the ground truth and predictions respectively. There should be 60 rows because each experiment was repeated 5 times, for values [1, 2, 5, 10, 15, 20]. So 2*5*6=60\n",
    "    # Therefore the func should be applied to each pair of rows.\n",
    "\n",
    "    metrics = {}\n",
    "    for name in training_name:\n",
    "        arr = results[experiment_type][name]\n",
    "        for i, size in enumerate([1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 20, 20, 20, 20, 20]):\n",
    "            ground_truth = arr[i*2]\n",
    "            predictions = arr[i*2+1]\n",
    "            if metrics.get(size) is None:\n",
    "                metrics[size] = []\n",
    "            metrics[size].append(func(ground_truth, predictions))\n",
    "    \n",
    "    if average:\n",
    "        for key in metrics.keys():\n",
    "            metrics[key] = np.mean(metrics[key])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def true_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def true_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def false_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "def false_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "\"\"\"\n",
    "def transient_overlap(y_true, y_pred):\n",
    "    # We'll do the following, set ground truth to 2 and predictions to 1.\n",
    "    sum_arr = y_true + y_pred\n",
    "    sum_arr[sum_arr == 2] = 1\n",
    "    # Find stretches where the result is non-zero\n",
    "    diff_arr = np.diff(sum_arr)\n",
    "    # Get pairs of indices where the difference is non-zero\n",
    "    diff_indices = np.where(diff_arr != 0)[0]\n",
    "    for i, idx in enumerate(diff_indices):\n",
    "        if i == 0 and diff_arr:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = lambda x, y: f1_score(x, y, average=\"macro\")\n",
    "f1_standard = lambda x, y: f1_score(x, y)\n",
    "tp = lambda x, y: true_positives(x, y)\n",
    "tn = lambda x, y: true_negatives(x, y)\n",
    "fp = lambda x, y: false_positives(x, y)\n",
    "fn = lambda x, y: false_negatives(x, y)\n",
    "precision = lambda x, y: precision_score(x, y)\n",
    "recall = lambda x, y: recall_score(x, y)\n",
    "accuracy = lambda x, y: accuracy_score(x, y)\n",
    "trans_TP = lambda x, y: transient_overlap(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m f1_result_macro \u001b[38;5;241m=\u001b[39m apply_to_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_animal\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1_macro, training_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPL010_D1S1\u001b[39m\u001b[38;5;124m\"\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m f1_result_standard \u001b[38;5;241m=\u001b[39m apply_to_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_animal\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1_standard, training_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPL010_D1S1\u001b[39m\u001b[38;5;124m\"\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m trans_TP_result \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcross_animal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_TP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPL010_D1S1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mtp_result = apply_to_results(\"within_session\", tp, average=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mtn_result = apply_to_results(\"within_session\", tn, average=True)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03maccuracy_result = apply_to_results(\"within_session\", accuracy, average=True)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mapply_to_results\u001b[1;34m(experiment_type, func, training_name, average)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 30\u001b[0m         metrics[key] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[1;32me:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "f1_result_macro = apply_to_results(\"cross_animal\", f1_macro, training_name=\"PL010_D1S1\", average=True)\n",
    "f1_result_standard = apply_to_results(\"cross_animal\", f1_standard, training_name=\"PL010_D1S1\", average=True)\n",
    "trans_TP_result = apply_to_results(\"cross_animal\", trans_TP, training_name=\"PL010_D1S1\", average=True)\n",
    "\"\"\"\n",
    "tp_result = apply_to_results(\"within_session\", tp, average=True)\n",
    "tn_result = apply_to_results(\"within_session\", tn, average=True)\n",
    "fp_result = apply_to_results(\"within_session\", fp, average=True)\n",
    "fn_result = apply_to_results(\"within_session\", fn, average=True)\n",
    "precision_result = apply_to_results(\"within_session\", precision, average=True)\n",
    "recall_result = apply_to_results(\"within_session\", recall, average=True)\n",
    "accuracy_result = apply_to_results(\"within_session\", accuracy, average=True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 macro\n",
      "{1: 0.5480958946232736, 2: 0.744130759185736, 5: 0.8227125097213384, 10: 0.8558539053757613, 15: 0.900605819452913, 20: 0.902093478833422}\n",
      "f1 standard\n",
      "{1: 0.10718292020987878, 2: 0.49587079979708165, 5: 0.6533831855177128, 10: 0.7182650480296602, 15: 0.8052980171893335, 20: 0.8084035209982664}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"True Positives Average\")\\nprint(tp_result)\\nprint(\"True Negatives Average\")\\nprint(tn_result)\\nprint(\"False Positives Average\")\\nprint(fp_result)\\nprint(\"False Negatives Average\")\\nprint(fn_result)\\nprint(\"Precision Average\")\\nprint(precision_result)\\nprint(\"Recall Average\")\\nprint(recall_result)\\nprint(\"Accuracy Average\")\\nprint(accuracy_result)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"f1 macro\")\n",
    "print(f1_result_macro)\n",
    "print(\"f1 standard\")\n",
    "print(f1_result_standard)\n",
    "\"\"\"\n",
    "print(\"True Positives Average\")\n",
    "print(tp_result)\n",
    "print(\"True Negatives Average\")\n",
    "print(tn_result)\n",
    "print(\"False Positives Average\")\n",
    "print(fp_result)\n",
    "print(\"False Negatives Average\")\n",
    "print(fn_result)\n",
    "print(\"Precision Average\")\n",
    "print(precision_result)\n",
    "print(\"Recall Average\")\n",
    "print(recall_result)\n",
    "print(\"Accuracy Average\")\n",
    "print(accuracy_result)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_exploration_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
