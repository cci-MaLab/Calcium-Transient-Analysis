{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract test results\n",
    "\n",
    "The following notebook is to extrapolate the data used in testing in the ./ml_training/test_results/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:37<00:00, 19.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Only set to True if you don't have the pickle file (LEAVE THIS FALSE IF YOU HAVE THE PICKLE FILE)\n",
    "if False:\n",
    "    results_path = \"./ml_training/test_results/\"\n",
    "\n",
    "    # Iterate through the results files and load the folders into dicts\n",
    "    results = {}\n",
    "    entries = os.listdir(results_path)\n",
    "    for entry in tqdm(entries):\n",
    "        if os.path.isdir(results_path + entry):\n",
    "            results[entry] = {}\n",
    "            sub_entries = os.listdir(results_path + entry)\n",
    "            for sub_entry in sub_entries:\n",
    "                if os.path.isdir(results_path + entry + \"/\" + sub_entry):\n",
    "                    results[entry][sub_entry] = {}\n",
    "                    sub_sub_entries = os.listdir(results_path + entry + \"/\" + sub_entry)\n",
    "                    # Extract the one file with .csv extension\n",
    "                    for sub_sub_entry in sub_sub_entries:\n",
    "                        if sub_sub_entry.endswith(\".csv\"):\n",
    "                            final_path = results_path + entry + \"/\" + sub_entry + \"/\" + sub_sub_entry\n",
    "                            results[entry][sub_entry] = np.genfromtxt(final_path, delimiter=\",\", dtype=float)\n",
    "                            # Greater than 0.5 is 1 result else 0\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] >= 0.5] = 1\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] < 0.5] = 0\n",
    "                            results[entry][sub_entry] = results[entry][sub_entry].astype(np.int8)\n",
    "                            break\n",
    "\n",
    "    # Save the dict with pickle\n",
    "    with open(\"test_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_animal\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_cross_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_same_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_session_same_day\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "within_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\n"
     ]
    }
   ],
   "source": [
    "results = pickle.load(open(\"test_results.pkl\", \"rb\"))\n",
    "# Print keys and subkeys\n",
    "for key in results.keys():\n",
    "    print(key)\n",
    "    for subkey in results[key].keys():\n",
    "        print(\"\\t\" + subkey)\n",
    "    print()\n",
    "\n",
    "print(\"The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes in three arguments, the experiment type, the training data and a function to apply to the data.\n",
    "# You can omit providing the training data and it will accumulate the results for all subkeys in the results dict.\n",
    "\n",
    "def apply_to_results(experiment_type, func, training_name=None, average=False):\n",
    "    if experiment_type not in results.keys():\n",
    "        raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "    if training_name is not None:\n",
    "        if training_name not in results[experiment_type].keys():\n",
    "            raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "        training_name = [training_name]\n",
    "    else:\n",
    "        training_name = results[experiment_type].keys()\n",
    "    \n",
    "    # We need to extract the results. It is set up in the following manner:\n",
    "    # Each two rows represent the ground truth and predictions respectively. There should be 60 rows because each experiment was repeated 5 times, for values [1, 2, 5, 10, 15, 20]. So 2*5*6=60\n",
    "    # Therefore the func should be applied to each pair of rows.\n",
    "\n",
    "    metrics = {}\n",
    "    for name in training_name:\n",
    "        arr = results[experiment_type][name]\n",
    "        for i, size in enumerate([1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 20, 20, 20, 20, 20]):\n",
    "            ground_truth = arr[i*2]\n",
    "            predictions = arr[i*2+1]\n",
    "            if metrics.get(size) is None:\n",
    "                metrics[size] = []\n",
    "            metrics[size].append(func(ground_truth, predictions))\n",
    "    \n",
    "    if average:\n",
    "        for key in metrics.keys():\n",
    "            metrics[key] = np.mean(metrics[key])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def true_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def true_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def false_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "def false_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "def _extract_indices(indices):\n",
    "        indices = indices.nonzero()[0]\n",
    "        if indices.any():\n",
    "            # Split up the indices into groups\n",
    "            indices = np.split(indices, np.where(np.diff(indices) != 1)[0]+1)\n",
    "            # Now Split the indices into pairs of first and last indices\n",
    "            indices = [(indices_group[0], indices_group[-1]+1) for indices_group in indices]\n",
    "\n",
    "        return indices\n",
    "\n",
    "def _overlaps(range1, range2):\n",
    "    return len(range(max(range1[0], range2[0]), min(range1[1], range2[1]))) != 0\n",
    "\n",
    "def transient_overlap(y_true, y_pred, threshold=0.5):    \n",
    "    indices_true = _extract_indices(y_true)\n",
    "    indices_pred = _extract_indices(y_pred)\n",
    "\n",
    "    # Now iterate through the indices and check if they overlap and if they do overlap what is the percentage of overlap\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # We'll iterate through both simultaneously and check their ranges, we'll pop from the list anything that is outside the range or already counted\n",
    "    while indices_true and indices_pred:\n",
    "        true_range = indices_true[0]\n",
    "        pred_range = indices_pred[0]\n",
    "\n",
    "        # Check if there is overlap\n",
    "        if _overlaps(true_range, pred_range):\n",
    "            # Check the percentage of overlap\n",
    "            overlap = len(range(max(true_range[0], pred_range[0]), min(true_range[1], pred_range[1])))\n",
    "            overlap_percentage = overlap / (pred_range[1] - pred_range[0])\n",
    "            if overlap_percentage >= threshold:\n",
    "                tp += 1\n",
    "                # Pop both indices\n",
    "                indices_true.pop(0)\n",
    "                indices_pred.pop(0)\n",
    "            elif true_range[1] < pred_range[1]:\n",
    "                indices_true.pop(0)\n",
    "                fn += 1\n",
    "            else:\n",
    "                indices_pred.pop(0)\n",
    "                fp += 1\n",
    "\n",
    "        else:\n",
    "            # Whichever ends first, pop it\n",
    "            if true_range[1] < pred_range[1]:\n",
    "                indices_true.pop(0)\n",
    "                fn += 1\n",
    "            else:\n",
    "                indices_pred.pop(0)\n",
    "                fp += 1\n",
    "        \n",
    "    # Now we need to add the remaining indices\n",
    "    fn += len(indices_true)\n",
    "    fp += len(indices_pred)\n",
    "\n",
    "    return (tp, fp, fn)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = lambda x, y: f1_score(x, y, average=\"macro\")\n",
    "f1_standard = lambda x, y: f1_score(x, y)\n",
    "tp = lambda x, y: true_positives(x, y)\n",
    "tn = lambda x, y: true_negatives(x, y)\n",
    "fp = lambda x, y: false_positives(x, y)\n",
    "fn = lambda x, y: false_negatives(x, y)\n",
    "precision = lambda x, y: precision_score(x, y)\n",
    "recall = lambda x, y: recall_score(x, y)\n",
    "accuracy = lambda x, y: accuracy_score(x, y)\n",
    "trans = lambda x, y: transient_overlap(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "f1_result_macro = apply_to_results(\"cross_animal\", f1_macro, training_name=\"PL010_D1S1\", average=True)\n",
    "f1_result_standard = apply_to_results(\"cross_animal\", f1_standard, training_name=\"PL010_D1S1\", average=True)\n",
    "\n",
    "tp_result = apply_to_results(\"within_session\", tp, average=True)\n",
    "tn_result = apply_to_results(\"within_session\", tn, average=True)\n",
    "fp_result = apply_to_results(\"within_session\", fp, average=True)\n",
    "fn_result = apply_to_results(\"within_session\", fn, average=True)\n",
    "precision_result = apply_to_results(\"within_session\", precision, average=True)\n",
    "recall_result = apply_to_results(\"within_session\", recall, average=True)\n",
    "accuracy_result = apply_to_results(\"within_session\", accuracy, average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal Lange\\AppData\\Local\\Temp\\ipykernel_323012\\581586931.py:70: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  while indices_true and indices_pred:\n"
     ]
    }
   ],
   "source": [
    "trans_TP_result = apply_to_results(\"cross_animal\", trans, training_name=\"PL010_D1S1\", average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [(0, 0, 156), (0, 0, 190), (0, 0, 129), (0, 0, 182), (89, 160, 74)], 2: [(18, 9, 126), (116, 94, 40), (65, 49, 138), (82, 59, 60), (76, 82, 67)], 5: [(124, 70, 61), (145, 54, 31), (112, 35, 66), (94, 55, 96), (85, 47, 84)], 10: [(110, 76, 27), (97, 74, 59), (97, 124, 80), (123, 23, 17), (141, 100, 63)], 15: [(136, 54, 20), (138, 41, 16), (141, 83, 26), (118, 62, 29), (153, 72, 31)], 20: [(132, 63, 14), (94, 67, 36), (150, 57, 40), (132, 94, 18), (119, 64, 15)]}\n"
     ]
    }
   ],
   "source": [
    "print(trans_TP_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1 macro\")\n",
    "print(f1_result_macro)\n",
    "print(\"f1 standard\")\n",
    "print(f1_result_standard)\n",
    "\"\"\"\n",
    "print(\"True Positives Average\")\n",
    "print(tp_result)\n",
    "print(\"True Negatives Average\")\n",
    "print(tn_result)\n",
    "print(\"False Positives Average\")\n",
    "print(fp_result)\n",
    "print(\"False Negatives Average\")\n",
    "print(fn_result)\n",
    "print(\"Precision Average\")\n",
    "print(precision_result)\n",
    "print(\"Recall Average\")\n",
    "print(recall_result)\n",
    "print(\"Accuracy Average\")\n",
    "print(accuracy_result)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_exploration_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
