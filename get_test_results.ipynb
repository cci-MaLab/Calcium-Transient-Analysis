{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract test results\n",
    "\n",
    "The following notebook is to extrapolate the data used in testing in the ./ml_training/test_results/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, roc_auc_score)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:37<00:00, 19.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Only set to True if you don't have the pickle file (LEAVE THIS FALSE IF YOU HAVE THE PICKLE FILE)\n",
    "if True:\n",
    "    results_path = \"./ml_training/test_results/\"\n",
    "\n",
    "    # Iterate through the results files and load the folders into dicts\n",
    "    results = {}\n",
    "    entries = os.listdir(results_path)\n",
    "    for entry in tqdm(entries):\n",
    "        if os.path.isdir(results_path + entry):\n",
    "            results[entry] = {}\n",
    "            sub_entries = os.listdir(results_path + entry)\n",
    "            for sub_entry in sub_entries:\n",
    "                if os.path.isdir(results_path + entry + \"/\" + sub_entry):\n",
    "                    results[entry][sub_entry] = {}\n",
    "                    sub_sub_entries = os.listdir(results_path + entry + \"/\" + sub_entry)\n",
    "                    # Extract the one file with .csv extension\n",
    "                    for sub_sub_entry in sub_sub_entries:\n",
    "                        if sub_sub_entry.endswith(\".csv\"):\n",
    "                            final_path = results_path + entry + \"/\" + sub_entry + \"/\" + sub_sub_entry\n",
    "                            results[entry][sub_entry] = np.genfromtxt(final_path, delimiter=\",\", dtype=float)\n",
    "                            # Greater than 0.5 is 1 result else 0\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] >= 0.5] = 1\n",
    "                            results[entry][sub_entry][results[entry][sub_entry] < 0.5] = 0\n",
    "                            results[entry][sub_entry] = results[entry][sub_entry].astype(np.int8)\n",
    "                            break\n",
    "\n",
    "    # Save the dict with pickle\n",
    "    with open(\"test_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_animal\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_cross_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_day_same_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "cross_session_same_day\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "within_session\n",
      "\tAA034_D1S1\n",
      "\tAA036_D2S1\n",
      "\tAA058_D1S1\n",
      "\tPL010_D1S1\n",
      "\n",
      "The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\n"
     ]
    }
   ],
   "source": [
    "results = pickle.load(open(\"test_results.pkl\", \"rb\"))\n",
    "# Print keys and subkeys\n",
    "for key in results.keys():\n",
    "    print(key)\n",
    "    for subkey in results[key].keys():\n",
    "        print(\"\\t\" + subkey)\n",
    "    print()\n",
    "\n",
    "print(\"The names listed above were used for training the models. Get in touch with Michal to find out what the test set was or you can figure it out from ml_running.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes in three arguments, the experiment type, the training data and a function to apply to the data.\n",
    "# You can omit providing the training data and it will accumulate the results for all subkeys in the results dict.\n",
    "\n",
    "def apply_to_results(experiment_type, func, training_name=None, average=False):\n",
    "    if experiment_type not in results.keys():\n",
    "        raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "    if training_name is not None:\n",
    "        if training_name not in results[experiment_type].keys():\n",
    "            raise ValueError(\"The experiment type provided is not in the results dict\")\n",
    "        training_name = [training_name]\n",
    "    else:\n",
    "        training_name = results[experiment_type].keys()\n",
    "    \n",
    "    # We need to extract the results. It is set up in the following manner:\n",
    "    # Each two rows represent the ground truth and predictions respectively. There should be 60 rows because each experiment was repeated 5 times, for values [1, 2, 5, 10, 15, 20]. So 2*5*6=60\n",
    "    # Therefore the func should be applied to each pair of rows.\n",
    "\n",
    "    metrics = {}\n",
    "    for name in training_name:\n",
    "        arr = results[experiment_type][name]\n",
    "        for i, size in enumerate([1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 20, 20, 20, 20, 20]):\n",
    "            ground_truth = arr[i*2]\n",
    "            predictions = arr[i*2+1]\n",
    "            if metrics.get(size) is None:\n",
    "                metrics[size] = []\n",
    "            metrics[size].append(func(ground_truth, predictions))\n",
    "    \n",
    "    if average:\n",
    "        for key in metrics.keys():\n",
    "            metrics[key] = np.mean(metrics[key])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def true_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def true_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred == target))\n",
    "\n",
    "def false_positives(y_true, y_pred, target=1):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "def false_negatives(y_true, y_pred, target=0):\n",
    "    return np.sum(np.logical_and(y_true == target, y_pred != target))\n",
    "\n",
    "def _extract_indices(indices):\n",
    "        indices = indices.nonzero()[0]\n",
    "        if indices.any():\n",
    "            # Split up the indices into groups\n",
    "            indices = np.split(indices, np.where(np.diff(indices) != 1)[0]+1)\n",
    "            # Now Split the indices into pairs of first and last indices\n",
    "            indices = [(indices_group[0], indices_group[-1]+1) for indices_group in indices]\n",
    "\n",
    "        return indices\n",
    "\n",
    "def _overlaps(range1, range2):\n",
    "    return len(range(max(range1[0], range2[0]), min(range1[1], range2[1]))) != 0\n",
    "\n",
    "def transient_overlap(y_true, y_pred, threshold=0.5):\n",
    "    # Not sure if this works as necessary avoid for the time being.    \n",
    "    indices_true = _extract_indices(y_true)\n",
    "    indices_pred = _extract_indices(y_pred)\n",
    "\n",
    "    # Now iterate through the indices and check if they overlap and if they do overlap what is the percentage of overlap\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # We'll iterate through both simultaneously and check their ranges, we'll pop from the list anything that is outside the range or already counted\n",
    "    while indices_true and indices_pred:\n",
    "        true_range = indices_true[0]\n",
    "        pred_range = indices_pred[0]\n",
    "\n",
    "        # Check if there is overlap\n",
    "        if _overlaps(true_range, pred_range):\n",
    "            # Check the percentage of overlap\n",
    "            overlap = len(range(max(true_range[0], pred_range[0]), min(true_range[1], pred_range[1])))\n",
    "            overlap_percentage = overlap / (pred_range[1] - pred_range[0])\n",
    "            if overlap_percentage >= threshold:\n",
    "                tp += 1\n",
    "                # Pop both indices\n",
    "                indices_true.pop(0)\n",
    "                indices_pred.pop(0)\n",
    "            elif true_range[1] < pred_range[1]:\n",
    "                indices_true.pop(0)\n",
    "                fn += 1\n",
    "            else:\n",
    "                indices_pred.pop(0)\n",
    "                fp += 1\n",
    "\n",
    "        else:\n",
    "            # Whichever ends first, pop it\n",
    "            if true_range[1] < pred_range[1]:\n",
    "                indices_true.pop(0)\n",
    "                fn += 1\n",
    "            else:\n",
    "                indices_pred.pop(0)\n",
    "                fp += 1\n",
    "        \n",
    "    # Now we need to add the remaining indices\n",
    "    fn += len(indices_true)\n",
    "    fp += len(indices_pred)\n",
    "\n",
    "    return (tp, fp, fn)\n",
    "\n",
    "\n",
    "def to_csv(metrics, filename):\n",
    "    # MAKE SURE THE FILENAME IS DESCRIPTIVE!!!\n",
    "    with open(filename, \"w\") as f:\n",
    "        # Convert to pandas dataframe where the column names are the keys and the rows are the values\n",
    "        df = pd.DataFrame.from_dict(metrics)\n",
    "        df.to_csv(f, index=False, lineterminator=\"\\n\")\n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = lambda x, y: f1_score(x, y, average=\"macro\")\n",
    "f1_standard = lambda x, y: f1_score(x, y)\n",
    "tp = lambda x, y: true_positives(x, y)\n",
    "tn = lambda x, y: true_negatives(x, y)\n",
    "fp = lambda x, y: false_positives(x, y)\n",
    "fn = lambda x, y: false_negatives(x, y)\n",
    "precision = lambda x, y: precision_score(x, y)\n",
    "recall = lambda x, y: recall_score(x, y)\n",
    "accuracy = lambda x, y: accuracy_score(x, y)\n",
    "trans = lambda x, y: transient_overlap(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "e:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "e:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "e:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "f1_result_macro = apply_to_results(\"cross_animal\", f1_macro, training_name=\"PL010_D1S1\", average=False)\n",
    "f1_result_standard = apply_to_results(\"cross_animal\", f1_standard, training_name=\"PL010_D1S1\", average=True)\n",
    "\n",
    "tp_result = apply_to_results(\"within_session\", tp, average=True)\n",
    "tn_result = apply_to_results(\"within_session\", tn, average=True)\n",
    "fp_result = apply_to_results(\"within_session\", fp, average=True)\n",
    "fn_result = apply_to_results(\"within_session\", fn, average=True)\n",
    "precision_result = apply_to_results(\"within_session\", precision, average=True)\n",
    "recall_result = apply_to_results(\"within_session\", recall, average=True)\n",
    "accuracy_result = apply_to_results(\"within_session\", accuracy, average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0.495326928109462, 0.4931821082903461, 0.4962271614521137, 0.49263542437920704, 0.7631078508852389], 2: [0.6203726974340997, 0.8336420110631643, 0.71163797253501, 0.790984696177297, 0.764016418719109], 5: [0.8499461083595601, 0.895305322430282, 0.7993298959296035, 0.7716141709164169, 0.7973670509708289], 10: [0.8933202110429579, 0.8126358084632079, 0.8041508284468021, 0.9304224477038816, 0.8387402312219573], 15: [0.9022295811866685, 0.934016268001163, 0.8888690529129128, 0.8820831353421288, 0.8958310598216916], 20: [0.9190141175894933, 0.8794123082899297, 0.9025038499722022, 0.8937177340512059, 0.9158193842642791]}\n"
     ]
    }
   ],
   "source": [
    "print(f1_result_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1_result_macro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1_macro_PL010_D1S1.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 110\u001b[0m, in \u001b[0;36mto_csv\u001b[1;34m(metrics, filename)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_csv\u001b[39m(metrics, filename):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# MAKE SURE THE FILENAME IS DESCRIPTIVE!!!\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Convert to pandas dataframe where the column names are the keys and the rows are the values\u001b[39;00m\n\u001b[0;32m    112\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(metrics)\n\u001b[0;32m    113\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_excel(f, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32me:\\milange\\miniconda\\envs\\cell_exploration_ml\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         path,\n\u001b[0;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "to_csv(f1_result_macro, \"f1_macro_PL010_D1S1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1 macro\")\n",
    "print(f1_result_macro)\n",
    "print(\"f1 standard\")\n",
    "print(f1_result_standard)\n",
    "\"\"\"\n",
    "print(\"True Positives Average\")\n",
    "print(tp_result)\n",
    "print(\"True Negatives Average\")\n",
    "print(tn_result)\n",
    "print(\"False Positives Average\")\n",
    "print(fp_result)\n",
    "print(\"False Negatives Average\")\n",
    "print(fn_result)\n",
    "print(\"Precision Average\")\n",
    "print(precision_result)\n",
    "print(\"Recall Average\")\n",
    "print(recall_result)\n",
    "print(\"Accuracy Average\")\n",
    "print(accuracy_result)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_exploration_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
