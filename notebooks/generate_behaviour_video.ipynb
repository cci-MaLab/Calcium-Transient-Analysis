{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate behaviour video\n",
    "\n",
    "You need to link two timestamp files, one will pertain to the framerate of the behaviour cam and the other of the calcium imaging video. We won't be synchronizing them framerate wise as long as they roughly finish and end at the same time we'll be fine. This has been setup to just work with avi files but you can look at the minian repo if you need a more comprehensive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Optional\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import dask as da\n",
    "import dask.array as darr\n",
    "from natsort import natsorted\n",
    "import _operator\n",
    "import ffmpeg\n",
    "import re\n",
    "import functools as fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minian_timestamps = \"/N/project/Cortical_Calcium_Image/Miniscope data/05.2023_Tenth_group/AA058_D1/AA058_D1_S1_behavior_ms.csv\"\n",
    "behavior_timestamps = \"/N/project/Cortical_Calcium_Image/Miniscope data/05.2023_Tenth_group/AA058_D1/2023_05_05/11_02_42/BehavCam_0/timeStamps.csv\"\n",
    "behavior_video_path = \"/N/project/Cortical_Calcium_Image/Miniscope data/05.2023_Tenth_group/AA058_D1/2023_05_05/11_02_42/BehavCam_0\"\n",
    "\n",
    "pattern = \"msCam[0-9]+\\.avi$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_FUNCTIONS = [\n",
    "    darr.core.getter_inline,\n",
    "    darr.core.getter,\n",
    "    _operator.getitem,\n",
    "    zr.core.Array,\n",
    "    darr.chunk.astype,\n",
    "    darr.core.concatenate_axes,\n",
    "    darr.core._vindex_slice,\n",
    "    darr.core._vindex_merge,\n",
    "    darr.core._vindex_transpose,\n",
    "]\n",
    "\n",
    "# Taken from minian repo\n",
    "def load_videos(\n",
    "    vpath: str,\n",
    "    pattern=r\"msCam[0-9]+\\.avi$\",\n",
    "    dtype: Union[str, type] = np.float64,\n",
    "    downsample: Optional[dict] = None,\n",
    "    downsample_strategy=\"subset\",\n",
    "    post_process: Optional[Callable] = None,\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Load multiple videos in a folder and return a `xr.DataArray`.\n",
    "\n",
    "    Load videos from the folder specified in `vpath` and according to the regex\n",
    "    `pattern`, then concatenate them together and return a `xr.DataArray`\n",
    "    representation of the concatenated videos. The videos are sorted by\n",
    "    filenames with :func:`natsort.natsorted` before concatenation. Optionally\n",
    "    the data can be downsampled, and the user can pass in a custom callable to\n",
    "    post-process the result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vpath : str\n",
    "        The path containing the videos to load.\n",
    "    pattern : regexp, optional\n",
    "        The regexp matching the filenames of the videso. By default\n",
    "        `r\"msCam[0-9]+\\.avi$\"`, which can be interpreted as filenames starting\n",
    "        with \"msCam\" followed by at least a number, and then followed by \".avi\".\n",
    "    dtype : Union[str, type], optional\n",
    "        Datatype of the resulting DataArray, by default `np.float64`.\n",
    "    downsample : dict, optional\n",
    "        A dictionary mapping dimension names to an integer downsampling factor.\n",
    "        The dimension names should be one of \"height\", \"width\" or \"frame\". By\n",
    "        default `None`.\n",
    "    downsample_strategy : str, optional\n",
    "        How the downsampling should be done. Only used if `downsample` is not\n",
    "        `None`. Either `\"subset\"` where data points are taken at an interval\n",
    "        specified in `downsample`, or `\"mean\"` where mean will be taken over\n",
    "        data within each interval. By default `\"subset\"`.\n",
    "    post_process : Callable, optional\n",
    "        An user-supplied custom function to post-process the resulting array.\n",
    "        Four arguments will be passed to the function: the resulting DataArray\n",
    "        `varr`, the input path `vpath`, the list of matched video filenames\n",
    "        `vlist`, and the list of DataArray before concatenation `varr_list`. The\n",
    "        function should output another valide DataArray. In other words, the\n",
    "        function should have signature `f(varr: xr.DataArray, vpath: str, vlist:\n",
    "        List[str], varr_list: List[xr.DataArray]) -> xr.DataArray`. By default\n",
    "        `None`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    varr : xr.DataArray\n",
    "        The resulting array representation of the input movie. Should have\n",
    "        dimensions (\"frame\", \"height\", \"width\").\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        if no files under `vpath` match the pattern `pattern`\n",
    "    ValueError\n",
    "        if the matched files does not have extension \".avi\", \".mkv\" or \".tif\"\n",
    "    NotImplementedError\n",
    "        if `downsample_strategy` is not \"subset\" or \"mean\"\n",
    "    \"\"\"\n",
    "    vpath = os.path.normpath(vpath)\n",
    "    vlist = natsorted(\n",
    "        [vpath + os.sep + v for v in os.listdir(vpath) if re.search(pattern, v)]\n",
    "    )\n",
    "    if not vlist:\n",
    "        raise FileNotFoundError(\n",
    "            \"No data with pattern {}\"\n",
    "            \" found in the specified folder {}\".format(pattern, vpath)\n",
    "        )\n",
    "    print(\"loading {} videos in folder {}\".format(len(vlist), vpath))\n",
    "\n",
    "    file_extension = os.path.splitext(vlist[0])[1]\n",
    "    if file_extension in (\".avi\"):\n",
    "        movie_load_func = load_avi_lazy\n",
    "    else:\n",
    "        raise ValueError(\"Extension not supported.\")\n",
    "\n",
    "    varr_list = [movie_load_func(v) for v in vlist]\n",
    "    varr = darr.concatenate(varr_list, axis=0)\n",
    "    varr = xr.DataArray(\n",
    "        varr,\n",
    "        dims=[\"frame\", \"height\", \"width\"],\n",
    "        coords=dict(\n",
    "            frame=np.arange(varr.shape[0]),\n",
    "            height=np.arange(varr.shape[1]),\n",
    "            width=np.arange(varr.shape[2]),\n",
    "        ),\n",
    "    )\n",
    "    if dtype:\n",
    "        varr = varr.astype(dtype)\n",
    "    if downsample:\n",
    "        if downsample_strategy == \"mean\":\n",
    "            varr = varr.coarsen(**downsample, boundary=\"trim\", coord_func=\"min\").mean()\n",
    "        elif downsample_strategy == \"subset\":\n",
    "            varr = varr.isel(**{d: slice(None, None, w) for d, w in downsample.items()})\n",
    "        else:\n",
    "            raise NotImplementedError(\"unrecognized downsampling strategy\")\n",
    "    varr = varr.rename(\"fluorescence\")\n",
    "    if post_process:\n",
    "        varr = post_process(varr, vpath, vlist, varr_list)\n",
    "    arr_opt = fct.partial(custom_arr_optimize, keep_patterns=[\"^load_avi_ffmpeg\"])\n",
    "    with da.config.set(array_optimize=arr_opt):\n",
    "        varr = da.optimize(varr)[0]\n",
    "    return varr\n",
    "\n",
    "\n",
    "def load_avi_lazy(fname: str) -> darr.array:\n",
    "    \"\"\"\n",
    "    Lazy load an avi video.\n",
    "\n",
    "    This function construct a single delayed task for loading the video as a\n",
    "    whole.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The filename of the video to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arr : darr.array\n",
    "        The array representation of the video.\n",
    "    \"\"\"\n",
    "    probe = ffmpeg.probe(fname)\n",
    "    video_info = next(s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\")\n",
    "    w = int(video_info[\"width\"])\n",
    "    h = int(video_info[\"height\"])\n",
    "    f = int(video_info[\"nb_frames\"])\n",
    "    return da.array.from_delayed(\n",
    "        da.delayed(load_avi_ffmpeg)(fname, h, w, f), dtype=np.uint8, shape=(f, h, w)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_avi_ffmpeg(fname: str, h: int, w: int, f: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load an avi video using `ffmpeg`.\n",
    "\n",
    "    This function directly invoke `ffmpeg` using the `python-ffmpeg` wrapper and\n",
    "    retrieve the data from buffer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The filename of the video to load.\n",
    "    h : int\n",
    "        The height of the video.\n",
    "    w : int\n",
    "        The width of the video.\n",
    "    f : int\n",
    "        The number of frames in the video.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arr : np.ndarray\n",
    "        The resulting array. Has shape (`f`, `h`, `w`).\n",
    "    \"\"\"\n",
    "    out_bytes, err = (\n",
    "        ffmpeg.input(fname)\n",
    "        .video.output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"gray\")\n",
    "        .run(capture_stdout=True)\n",
    "    )\n",
    "    return np.frombuffer(out_bytes, np.uint8).reshape(f, h, w)\n",
    "\n",
    "def custom_arr_optimize(\n",
    "    dsk: dict,\n",
    "    keys: list,\n",
    "    fast_funcs: list = FAST_FUNCTIONS,\n",
    "    inline_patterns=[],\n",
    "    rename_dict: Optional[dict] = None,\n",
    "    rewrite_dict: Optional[dict] = None,\n",
    "    keep_patterns=[],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Customized implementation of array optimization function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dsk : dict\n",
    "        Input dask task graph.\n",
    "    keys : list\n",
    "        Output task keys.\n",
    "    fast_funcs : list, optional\n",
    "        List of fast functions to be inlined. By default :const:`FAST_FUNCTIONS`.\n",
    "    inline_patterns : list, optional\n",
    "        List of patterns of task keys to be inlined. By default `[]`.\n",
    "    rename_dict : dict, optional\n",
    "        Dictionary mapping old task keys to new ones. Only used during fusing of\n",
    "        tasks. By default `None`.\n",
    "    rewrite_dict : dict, optional\n",
    "        Dictionary mapping old task key substrings to new ones. Applied at the\n",
    "        end of optimization to all task keys. By default `None`.\n",
    "    keep_patterns : list, optional\n",
    "        List of patterns of task keys that should be preserved during\n",
    "        optimization. By default `[]`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dsk : dict\n",
    "        Optimized dask graph.\n",
    "\n",
    "    See Also\n",
    "    -------\n",
    "    :doc:`dask:optimize`\n",
    "    `dask.array.optimization.optimize`\n",
    "    \"\"\"\n",
    "    # inlining lots of array operations ref:\n",
    "    # https://github.com/dask/dask/issues/6668\n",
    "    if rename_dict:\n",
    "        key_renamer = fct.partial(custom_fused_keys_renamer, rename_dict=rename_dict)\n",
    "    else:\n",
    "        key_renamer = custom_fused_keys_renamer\n",
    "    keep_keys = []\n",
    "    if keep_patterns:\n",
    "        key_ls = list(dsk.keys())\n",
    "        for pat in keep_patterns:\n",
    "            keep_keys.extend(list(filter(lambda k: check_key(k, pat), key_ls)))\n",
    "    dsk = darr.optimization.optimize(\n",
    "        dsk,\n",
    "        keys,\n",
    "        fuse_keys=keep_keys,\n",
    "        fast_functions=fast_funcs,\n",
    "        rename_fused_keys=key_renamer,\n",
    "    )\n",
    "    if inline_patterns:\n",
    "        dsk = inline_pattern(dsk, inline_patterns, inline_constants=False)\n",
    "    if rewrite_dict:\n",
    "        dsk_old = dsk.copy()\n",
    "        for key, val in dsk_old.items():\n",
    "            key_new = rewrite_key(key, rewrite_dict)\n",
    "            if key_new != key:\n",
    "                dsk[key_new] = val\n",
    "                dsk[key] = key_new\n",
    "    return dsk\n",
    "\n",
    "def check_key(key: Union[str, tuple], pat: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether `key` contains pattern.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : Union[str, tuple]\n",
    "        Input key. If a `tuple` then the first element will be used to check.\n",
    "    pat : str\n",
    "        Pattern to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether `key` contains pattern.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return bool(re.search(pat, key))\n",
    "    except TypeError:\n",
    "        return bool(re.search(pat, key[0]))\n",
    "\n",
    " \n",
    "def rewrite_key(key: Union[str, tuple], rwdict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite a task key according to `rwdict`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : Union[str, tuple]\n",
    "        Input task key.\n",
    "    rwdict : dict\n",
    "        Dictionary mapping old task key substring to new ones. All keys in this\n",
    "        dictionary that exists in input `key` will be substituted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    key : str\n",
    "        The new key.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if input `key` is neither `str` or `tuple`\n",
    "    \"\"\"\n",
    "    typ = type(key)\n",
    "    if typ is tuple:\n",
    "        k = key[0]\n",
    "    elif typ is str:\n",
    "        k = key\n",
    "    else:\n",
    "        raise ValueError(\"key must be either str or tuple: {}\".format(key))\n",
    "    for pat, repl in rwdict.items():\n",
    "        k = re.sub(pat, repl, k)\n",
    "    if typ is tuple:\n",
    "        ret_key = list(key)\n",
    "        ret_key[0] = k\n",
    "        return tuple(ret_key)\n",
    "    else:\n",
    "        return k\n",
    "    \n",
    "\n",
    "def inline_pattern(dsk: dict, pat_ls: List[str], inline_constants: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Inline tasks whose keys match certain patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dsk : dict\n",
    "        Input dask graph.\n",
    "    pat_ls : List[str]\n",
    "        List of patterns to check.\n",
    "    inline_constants : bool\n",
    "        Whether to inline constants.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dsk : dict\n",
    "        Dask graph with keys inlined.\n",
    "\n",
    "    See Also\n",
    "    -------\n",
    "    dask.optimization.inline\n",
    "    \"\"\"\n",
    "    keys = [k for k in dsk.keys() if check_pat(k, pat_ls)]\n",
    "    if keys:\n",
    "        dsk = inline(dsk, keys, inline_constants=inline_constants)\n",
    "        for k in keys:\n",
    "            del dsk[k]\n",
    "        if inline_constants:\n",
    "            dsk, dep = cull(dsk, set(list(flatten(keys))))\n",
    "    return dsk\n",
    "\n",
    "def check_pat(key: Union[str, tuple], pat_ls: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether `key` contains any pattern in a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : Union[str, tuple]\n",
    "        Input key. If a `tuple` then the first element will be used to check.\n",
    "    pat_ls : List[str]\n",
    "        List of pattern to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether `key` contains any pattern in the list.\n",
    "    \"\"\"\n",
    "    for pat in pat_ls:\n",
    "        if check_key(key, pat):\n",
    "            return True\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_exploration_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
